# Unsupervised Natural Language Parsing (EACL 2021 Tutorial)
Presented by [Kewei Tu](http://faculty.sist.shanghaitech.edu.cn/faculty/tukw/), [Yong Jiang](http://jiangyong.site/), [Wenjuan Han](http://hanwenjuan.com/), [Yanpeng Zhao](https://zhaoyanpeng.github.io/)

## Overview
Unsupervised parsing learns a syntactic parser from training sentences without parse tree annotations. Recently, there has been a resurgence of interest in unsupervised parsing, which can be attributed to the combination of two trends in the NLP community: a general trend towards unsupervised training or pre-training, and an emerging trend towards finding or modeling linguistic structures in neural models.

In this tutorial, we will introduce to the general audience what unsupervised parsing does and how it can be useful for and beyond syntactic parsing. We will then provide a systematic overview of major classes of approaches to unsupervised parsing, namely generative and discriminative approaches, and analyze their relative strengths and weaknesses. We will cover both decade-old statistical approaches and more recent neural approaches to give the audience a sense of the historical and recent development of the field. We will also discuss emerging research topics such as BERT-based approaches and visually grounded learning.

## Outline
1. [Introduction](Part%201%20Introduction.pdf)
2. Generative Approaches ([part a](Part%202%20Generative%20Approaches%20(a).pdf))([part b](Part%202%20Generative%20Approaches%20(b).pdf))
3. [Discriminative Approaches](Part%203%20Discriminative%20Approaches.pdf)
4. [Special Topics](Part%204%20Special%20Topics.pdf)
5. [Summary](Part%205%20Summary.pdf)
6. References (to be added)
